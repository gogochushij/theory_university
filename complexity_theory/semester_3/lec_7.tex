\newpage
\lecture{7}{17 dec}{\dag}
\section{Вероятностные вычисления}
\subsection{Классы с односторонней ошибкой}
\begin{defn}[Класс $ \RP$]\index{\RP}
	$ L \in \RP$, если существует п.о.п.п. отношение $ R$ такое, что $ \forall x \in \{0, 1\}^{*}$ :
	\[
		\begin{aligned}
		&x \notin L && \Longrightarrow \forall w \colon (x, w) \notin R \\
		&x \in L && \Longrightarrow \frac{ \lvert \{w \mid (x, w) \in  R\} \rvert }{\lvert \{\text{все } w\} \rvert } > \frac{1}{2}
		\end{aligned}
	\]
\end{defn}
\begin{defn}[Классы без ошибки]\index{\ZPP}
	$ \ZPP = \RP \cap \coRP$.
\end{defn}

\begin{defn}[Классы с двусторонней ошибкой]\index{\BPP}
	$ L \in \BPP$, если существует п.o.п.п. отношение $ R$ такое, что $ \forall x \in \{0, 1\}^{*}$ :
	\[
		\begin{aligned}
		&x \notin L && \Longrightarrow \frac{ \lvert \{w \mid (x, w) \in  R\} \rvert }{\lvert \{\text{все } w\} \rvert } < \frac{1}{3}\\
		&x \in L && \Longrightarrow \frac{ \lvert \{w \mid (x, w) \in  R\} \rvert }{\lvert \{\text{все } w\} \rvert } > \frac{2}{3}
		\end{aligned}
	\]
\end{defn}

\begin{lm}
	Пусть некоторый язык принадлежит $ \RP$. Запустим соответствующий алгоритм $ k$ раз. Тогда
	\[
		\Pr[k \text{ неудач}] \le \frac{1}{2^{k}}
	.\]
\end{lm}
\begin{proof}
	Очевидно
\end{proof}
\begin{lm}
	Пусть некоторый язык принадлежит $ \BPP$. Запустим соответствующий алгоритм $ k$ раз, вернем самый частый ответ. Тогда
	\[
		\Pr[\text{ошибок более } \tfrac{k}{2}] \le \frac{1}{2^{\Omega (k)}}
	.\]
\end{lm}
\begin{proof}
	\begin{st}[Неравенство Чернова, без доказательства]
		\[
			\Pr[X > (1+\varepsilon ) p k] < \left( \frac{e^{\varepsilon }}{(1+\varepsilon)^{1+\varepsilon } } \right) ^{p k} \le e^{-\frac{pk \varepsilon ^2}{4}}
		,\]
		где $ X = \sum_{i=1}^{k} x_i$, $ x_{i}$ --- независимые случайные величины, принимающие $ 1$ с вероятностью $ p$ и $ 0$ с вероятностью $ 1-p$.
	\end{st}
	В данной задаче $ x_i$ --- наличие ошибки при $ i$-ом вычислении, $ p = \frac{1}{3}$, $ \varepsilon  = \frac{1}{2}$.
\end{proof}

\begin{defn}[Альтернативное определение $ \ZPP$]\index{\ZPP}
	$ \ZPP$ --- алгоритмы без ошибок с полиномиальным матожиданием времени работы.
\end{defn}
\begin{proof}
	\begin{itemize}
		\item
			Пусть есть два алгоритма: один $ \RP$, второй $ \coRP$. Пока первый выдает  $ 0$, мы не уверены в ответе, и, пока второй выдает $ 1$ мы не уверены.

			Когда-то один из них выдаст правильный ответ.
			Тогда матожидание равно
			\[
				\E = \O\left( (\tfrac{1}{2} + 2\cdot \tfrac{1}{4}+ 3\cdot \tfrac{1}{8} + 4\cdot \tfrac{1}{16} + \ldots ) t(n)\right)  = \O(t(n))
			,\]
			где $ t(n)$ --- время самого долгого из алгоритмов.
		\item Пусть есть алгоритм $A $, для которого $ \E t_{A}(n) \le p(n)$.

			Запустим его  $ k \cdot  p(n)$ раз и прервем.
			Тогда вероятность неверного ответа по неравенству МАРКОВА, меньше~$ \frac{1}{k}$, а время на выполнения все еще полиномиально.
	\end{itemize}
\end{proof}

\subsection{Связь с другими классами}
\begin{thm}
	$ \BPP \subset \Ppoly$.
\end{thm}
\begin{proof}
	\begin{itemize}
		\item
			Вероятностный алгоритм дополнительно получает случайную строку (подсказку)\footnote{Всегда можно считать, что ее длина не больше времени работы, так как дальше мы ее просто не прочитаем.
			Еще можно считать, что длины всех случайных срок равны.}.
			
			Назовем ее \textit{хорошей} подсказкой для входа $ x$, если она приводит к правильному ответу, и \textit{плохой} иначе.

			Давайте уменьшим ошибку до $ \frac{1}{4^{n}}$. %Тогда доля хороших будет равна $ 1 - \frac{1}{4^{n}}$.

		\item
			Хотелось бы найти хорошую строку для всех входов $ x$. Входов длины $ n$ всего $ 2^{n}$ штук.

			Заметим, что $ \frac{1}{4^{n}} \cdot 2^{n} < 1$, поэтому доля плохих строк для всех входов точно меньше единицы, поэтому есть хорошая для всех входов.

		\item
			Построим булеву схему по нашему алгоритму.

			$ \BPP$ задается полиномиальным алгоритмом, у которого два аргумента: вход и случайная строка.
			Тогда это просто НМТ. По теореме Кука-Левина мы можем построить булеву схему, соответствующую НМТ.
			\begin{figure}[ht]
				\centering
				\incfig{random-bpp}
				\caption{Булева схема для $ \BPP$}
				\label{fig:random-bpp}
			\end{figure}
			У этой схемы есть часть входов для $ x$ и часть для случайной строки $ w$. Просто зашьем вместо  $ w$ одну из хороших для всех входов строку.
	\end{itemize}
\end{proof}

\begin{thm}
	$ \BPP \subseteq \SIGMA^2\P$.
\end{thm}
\begin{proof}
	Пусть есть язык $ R \in \BPP$. Пусть вероятность ошибки мы уже уменьшили до $ 2^{-n}$.
	\[
		A_x = \{w \in \{0, 1\}^{p(n)} \mid R(x, w) = 1\}
	.\]
	Построим формулу с двумя кванторами.

	\begin{itemize}
		\item $ x \in L$.
			Будем покрывать все возможные случаи строки $ U = \{0, 1\}^{p(n)}$ копиями $ A_x$.

			Если мы покроем полиномиальным количеством копий, то получим следующее:
			\[
				\exists \{t_{i}\}_{i=1}^{k} ~ \forall r \in U ~ \bigvee_{i=1}^{k} (r \in A_x \oplus t_i)
			.\]
			\begin{figure}[ht]
				\centering
				\incfig{copy-ax}
				\caption{Покрытие случайных строк}
				\label{fig:copy-ax}
			\end{figure}
			После первого квантора полином битов, после второго тоже, поэтому, чтобы данная схема была из $ \SIGMA^2\P$, нужно проверить, что все вычисления работают полином.
		\item $ x \notin L$.
			В этом случае за полином $ A_x$ мы не покроем все $ U$, так как их доля $ \frac{1}{2^{n}}$.
	\end{itemize}
	Для первого случая осталось доказать, что, во-первых, можно найти полиномиальное множество $ \{t_i\}$, а, во-вторых, вычисления можно провести за полиномиальное время.
	\begin{enumerate}
	    \item 
			Выберем случайно множество $ \{t_i\}_{i}$ и убедимся, что вероятность полного покрытия больше нуля.

			Запишем вероятность того, что мы не покрыли все строки:
			\[
			\begin{aligned}
				\Pr \left[ \neg \left( \forall r \in U ~\bigvee_{i=1}^{k}(r \in A_x \oplus t_i) \right)  \right] &= \Pr \left[ \exists r \in U ~ \bigwedge_{i=1}^{k}\left( r \notin A_x \oplus t_i \right)  \right] \le \\
																												 & \le \sum_{r \in U}^{} \Pr \left[ \bigwedge_{i=1}^{k} (r \notin A_x \oplus t_i) \right]  \stackrel{\text{выбирали независимо}}{=} \sum_{r \in U}^{} \prod_{i=1}^{k} \Pr [r \notin A_x \oplus t_i] \le \\
																												 & \le \frac{1}{2^{nk}}\cdot 2 ^{p(n)}
			\end{aligned}
			\]
			Чтобы сделать $ \frac{2^{p(n)}}{2^{nk}} < 1$, возьмем, например, $k= p(n)$.
		\item Посчитать $ \vee$ для $ k$ значений легко. 
			Заметим, что 
			\[
			r \in A_x \oplus t_i \Longleftrightarrow r \oplus t_i \in A_x
			.\] 
			Чтобы выяснить последнее нужно запустить $ R(x, r \oplus t_i)$, так как $ A_x$ --- как раз строки, на которых $ R$ выдает  единицу.

			$ R$ работает полином, следовательно, все вычисления будут работать тоже полином.
	\end{enumerate} 
\end{proof}
\begin{note}
	Так как $ \BPP = \coBPP$, $ \BPP \subseteq \PI^2\P$, а тогда $ \BPP \subseteq \SIGMA^2\P \cap \PI^2\P$.
\end{note}


\section{Общая картина}

Ничего не известно про связь $\BPP$ c $\NP$ и $\coNP$

\begin{figure}[ht]
    \centering
    \incfig{general-picture}
    \caption{Общая картина иерархии}
    \label{fig:general-picture}
\end{figure}

\section{Квантовые вычисления}
\subsubsection{Недетерминированные МТ}
Посмотрим на вычисления как на матрицы: $ A(u, v) = 1 \Longleftrightarrow u \stackrel{\text{шаг МТ}}{\longrightarrow} v$.

Заметим, что не любая матрица подойдет под описание МТ. Это объясняется тем, что при переходе от одной конфигурации меняется константное число битов и один символ на ленте. И поэтому много участков памяти остаются прежними.

\subsubsection{Детерминированные МТ}
Мы аналогично можем записать все в матрицу, но в каждой строке будет ровно одна единица ($ \sum_{j}^{} a_{ij} = 1$), так как мы точно должны знать, куда попадем на следующем шаге.

Текущее состояние можно записать как строку $ x = (0, \ldots 0, 1, 0, \ldots 0)$, где $ 1$ на $ i$-ом месте говорит, что мы сейчас в $ i$-ом состоянии.

Теперь мы можем умножать транспонированный вектор на матрицу и получать следующее состояние.
\[
x \longmapsto xA \longmapsto xA^2 \longmapsto \ldots 
.\] 

\subsubsection{Вероятностные МТ}
С вероятностными МТ можно в матрице писать в возможных переходах писать вероятность $ \tfrac{1}{2}$ (так как у нас случайные биты, всего два варианта развития событий).

Аналогично, $ \sum_{j}^{} a_{ij} = 1$.  Такая матрица называется \selectedFont{стохастической}.
\[
\begin{aligned}
	&\delta (q, a, 0) = (q', b , \to / \leftarrow  ) \\
	& \delta (q, a, 1) = (q'', c, \to / \leftarrow  )
\end{aligned}
\]
Теперь можно думать о состоянии, как о <<сумме состояний>>: сумма произведений вероятности на прошлое состояние. 

\subsubsection{Квантовые МТ}
Теперь в матрице записаны некоторые комплексные числа, которые называются \selectedFont{амплитудами}. 

Матрица должна быть унитарной, $ A A^{*} = A^{*} A = E$. 

Наше текущее положение дел также записано в строку матрицы.

\selectedFont{Смешанное состояние} --- линейная комбинация чистых конфигураций с амплитудами
\[
\sum_{i}^{} x_i \cdot s_i
.\] 

Вероятность --- $ \lvert x_i \rvert ^2$, это примерно длина вектора.

Эволюция происходит аналогично, домножаем строку на матрицу.
Эволюция происходит пока мы не захотим получить ответ.

Матрица все равно должна быть получена из МТ или другого устройства, мы не можем осуществлять какие-то нелогичные переходы, не понятна их реализация.

Преобразования тоже должны быть унитарными с небольшим числом бит.

О квантовых МТ можно думать, как о булевых схемах с маленькими унитарными преобразованиями. 
\begin{ex}
	Пусть таким переходом будет матрица $ 2\times 2$, применяем к $ (1, 0)$. 
	\[
		\begin{pmatrix}
			1 & 0
		\end{pmatrix}
	\begin{pmatrix}
		\frac{1}{\sqrt{ 2} }& \frac{1}{\sqrt{ 2} }\\
		\frac{1}{\sqrt{ 2} }& -\frac{1}{\sqrt{ 2} }
	\end{pmatrix}
	.\] 
	Результатом будет $ 1$ с вероятностью $ \tfrac{1}{2}$ и $ 0$ с вероятностью $ \frac{1}{2}$.

	Если же применить дважды, так как матрица обратная к себе, получим только $ 0$.
\end{ex}

\begin{defn}\index{\BQP}
$ \BQP$ --- класс решаемых квантовым компьютером за полиномиальное время  с ограниченной ошибкой.
\end{defn}
