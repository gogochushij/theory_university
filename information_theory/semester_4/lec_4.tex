\lecture{4}{22 April}{\dag}
% Эта тема плохо написана в прошлогоднем конспекте
\section{Кодирование с ошибками}
Пусть есть алфавит $ \Sigma$ размером  $ k$, <<кодер>>  $ E \colon [k]^{n} \to  \{0, 1\}^{L_n}$ и <<декодер>> $ D\colon \{0, 1\}^{L_n} \to [k]^{n}$.

Пусть есть распределение на буквах $  p_1, p_2, \ldots p_k$. \footnote{считаем, что слово состоит из независимых букв}

Откажемся от условия однозначного декодирования, но будем требовать, чтобы 
 $ \varepsilon _n \coloneqq \Pr [D(E(x)) \ne x]$, где $\lvert x  \rvert = n$ стремилось к нулю $ \varepsilon _n \to  0$ при $ n \to  \infty$.\footnote{Если сделать равенство нулю, то особого сжатия не будет}


\begin{thm}
	\begin{enumerate}
		\item Если $ L_n > \lceil h n \rceil $, где  $ h > H(p)$, то кодирование есть, то есть существуют такие функции $ E$ и  $ D$, что  $ \varepsilon _n \to 0$.
		\item Если  $ L_n < \lceil hn \rceil$, где $ h < H(p)$, то  $ \varepsilon _n \to 1$ для любых $ E$ и $ D$.
	\end{enumerate}
\end{thm}
\begin{proof}
Зафиксируем $  \delta   = n^{-0.02}$.
	\begin{defn}
		Будем называть слово $ w$  \selectedFont{$ \delta $-типичным}, если 
	\[
	\forall i \left| \frac{n_i}{n} - p_i \right| \le \delta, \quad n_i = \#\text{входа буквы } i 
	.\] 
	\end{defn}
\begin{itemize}
\item Докажем, что можем закодировать такие типичные слова.
\begin{itemize}
\item Пусть $ X_{ij}$ --- характеристическая функция того, что в слове на позиции $ j$ находится буква  $ i$.

Для случайной величины $ X_i \coloneqq  \sum\limits_{j} X_{ij}$ применим неравенство Чебышева:\footnote{Здесь $ \Var[X_i]$ --- дисперсия.}
 \[
	 \Pr [ \lvert X_i - \mu \rvert \ge \delta  n ] \le  \frac{\Var  [X_i]}{( \delta  n) ^2} =
	 \frac{np_i (1-p_i)}{( \delta  n)^2} =  \O\left(\tfrac{1}{ \delta^2  n}\right)
 ,\]  где $ \mu = \E X_i=  n p_i$

 Так как букв константное количество, вероятность нетипичности все равно останется очень маленьким и будет стремиться к нулю.

\item Теперь докажем, что типичных слов не очень много. Количество слов, где буквы встречаются в количествах $  n_1, \ldots , n_k$ равно
\[
	N_{n_1, \ldots m_k} = \frac{n!}{n_1! \cdot \ldots \cdot n_k!}
.\] 

Обозначим $\delta_i = \frac{n_i}{n} - p_i$.
\begin{align*}
	\log N &= \tag{так как $n! = \poly(n) \left( \frac{n}{e} \right) ^{n }$}\\ 
		   &= \log \left( \left( \frac{n}{n_1} \right)^{n_1} \cdot \left( \frac{n}{n_2} \right)^{n_2} \cdot \ldots \cdot \left( \frac{n}{n_k} \right)^{n_k} \right)  + \O( \log n)=  \\
		   & =\sum_{i} n_i \log \frac{n}{n_i} + \O(\log n) = \tag{$n_i$ по определению}\\
		   &= n \sum_{i} (p_i + \delta _i) \cdot \log \frac{1}{p_i + \delta _i} + \O(\log n) 
\end{align*}
% В последнем переходе $ \lvert   \delta _i \rvert< \delta $, так как типичное, $  \delta _i$ --- отклонение $i $-ой буквы в языке

\item Теперь оценим число типичных слов. 

	Для этого можно посчитать для каждого распределения букв, сколько слов с таким распределением будут типичными.

	Число разбиений грубо оценивается сверху как $n^k$. А число слов в соответствующем разбиении можно оценить максимумом по всем $\delta _i$,  таким что $\lvert\delta _i\rvert \le \delta$ (так как слово $\delta$-типичное).
\begin{align*}
	\log\Bigl(	\# (\text{$\delta$-типичных слов})\Bigr) 
	& \le \log \left( n^{k} \cdot \max_{ \delta _i} N \right) \le \\
	& \le \max_{ \delta _i} H(p_1 + \delta _1 , p_2+\delta_2 , \ldots ) \cdot n + \O( \log n) = \tag{Переход за кадром\footnote{Написать честную формулу энтропии как частную производную}} \\
	& \le n \cdot \max \sum_i (p_i + \delta _i) \cdot \log \frac{1}{p_i + \delta _i} + \O( \log n) \le \\
	& \le n \cdot \left(\max \sum_i p_i \log \frac{1}{p_i} + k\delta \max \log\frac{1}{p_i}\right) + \O(\log n) = \\
	&= nH(p) + \O( \delta \cdot n)
\end{align*}
Если теперь <<кодер>> может отобразить инъективно все типичные слова в набор битовых слов длины $ h n$, при этом ошибаться он будет на нетипичных, количество которых стремиться к нулю.
\end{itemize}
 \item Во второй части докажем, что вероятность ошибки декодера на $\delta$-типичныx словах равна 1. 
	 Понятно, что этого будет достаточно.

	 Покажем, что вероятность того, что мы выкинем $ \delta$-типичное слово очень мала.
% \begin{figure}[ht]
%     \centering
%     \incfig{delta-words}
%     \label{fig:delta-words}
% \end{figure}

Пусть $ L_n \le h n$.
Посмотрим на любое кодовое распределение слов. Покажем, что вероятность по нашему определению для $  \delta $-типичных слов больше, чем $ \frac{1}{2^{L_n}}$.
\begin{align*}
	Pr[w] &= p_1^{n_1} \cdot p_2^{n_2} \cdot  \ldots \cdot p_k^{n_k} = \\
		  & = 2^{- \sum_{i=1}^{k} (p_i + \delta) \log \frac{1}{p_i} n}  \le \\
		  & \le 2^{-H(p) n + \O( \delta n)}
\end{align*}
С какой вероятностью декодер декодер ответит правильно?
\begin{align*}
	\Pr[D(E(w)) = w] &= \sum_{x}\Pr[D(x) = \\
					  &=w \mid E(w) = x] \cdot \Pr[E(w)=x] \le \sum_{x}\Pr[E(w)=x] \le \\
					  &\le 2^{L_n} \cdot \max_{w} \Pr[w] \le 2^{((L_n - H(p)) \cdot n + \O( \delta n)} \to 2^{0}
\end{align*}
\end{itemize}
\end{proof}
