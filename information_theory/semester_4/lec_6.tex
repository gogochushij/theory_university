\section{Теория информации в коммуникационной сложности}
\lecture{6}{6 May}{\dag}
\subsection{Подсчет функции индексов}
\begin{defn}
Определим \selectedFont{функцию индексирования} следующим образом:
 \[
	\Ind \colon [n]\times \{0, 1\}^{n} \to \{0, 1\}, \qquad \Ind(x, y) = y_x
.\] 
\end{defn}
Алиса и Боб хотят посчитать эту величину, причем $ x$ у Алисы, а  $ y $ у Боба.

Пусть сообщения идут только от Боба до Алисы. Несложно понять, что Бобу придется послать всю информацию.

Теперь предположим, что они хотят, чтобы Алиса посчитала $ \Ind$  верно с вероятностью $ \frac{1}{2} + \delta $, если $ x$ и $ y$ выбираются равномерно. Очевидно, для $  \delta  = 0$, просто ничего не нужно пересылать. А вот для других положительных значений все испортится.

\dotfill

Пусть $ M(y)$ --- случайная величина, это сообщение, которое передает Боб Алисе в зависимости от своего входа. Легко получить
\begin{align*}
    D(\pi) \ge \log \lvert L \rvert  \ge H(M) \ge I(M : y)
\end{align*}
Где $L$ --- множество различных сообщений, которые может отправить Боб (то есть множество различных листьев в протоколе), $ D(\pi)$ --- глубина протокола (то есть количество бит, которое нужно отправить). 
\begin{align*}
	I(M : y)& = \tag{Chain rule} 
			\sum_{i} I(M: y_i \mid y_{<i}) = \\
			&= \sum_i H(y_i \mid y_{<i}) - H(y_i \mid M, y_{<i}) = \tag{$ y_i$ независимы} \\
			&= \sum_i H(y_i) - H(y_i \mid M, y_{<i}) \ge \tag{Выкинули часть условий} \\
			& \ge \sum_i H(y_i) - H(y_i \mid M) = \\
			&= \sum I(M : y_i)
\end{align*}

Покажем, что полученная сумма большая.

Зафиксируем $ i$ и распишем по определению взаимной информации: 
\begin{align*}
	\sum_{i} I(M : y_i) &= \sum_{i} H(y_i) - H(y_i \mid M) = \tag{$ H(y_i) = 1$} \\
	           &= \sum_{i}  1 - \E _{m} \left( H(y_i \mid M= m) \right) =\\\tag{$x$ не зависит от $(M, y)$, значит $(y\mid M)$ не зависимо с $(x \mid M)$}\\
			   &= \sum_{i} 1 - \E _{m} \left( H(y_i \mid M= m, x= i) \right)  = \\
			   &= \sum_{i} 1 - \E_{m}(H(r_m^{i})) \ge \tag{Неравенство Йенсена для выпуклой вверх $ H(q)$} \\
			   &\ge \sum_{i} 1 - H(\E_{m}(r_m^{i})) = \\
			   &= n - n \sum_{i}^{} \frac{1}{n} H(\E_{m}(r_m^{i})) \ge \tag{Опять неравенство Йенсена для $ H(q)$}\\
			   &\ge n \cdot \Big(1 - H\big(\E_{m, i} (r_m^{i})\big)\Big) \ge \tag{Монотонность энтропии на $(0, \tfrac{1}{2})$} \\
			   & \ge n \cdot \bigg(1 - H\Big(\tfrac{1}{2} - \delta \Big)\bigg) = \tag{Ряд Тейлора для энтропии}\\
			   &= \Omega(\delta ^2 n)
\end{align*}
Здесь $ r_m^{i} $ --- характеристическая функция ошибки  $ M=m$,  $ x = i$. 

\noindent\textit{Пояснение к переходу к  $ r_m^{i} $}:
Так как у Алисы алгоритм детерминированный, то при фиксированных $x, M$ она решает, что $y_x$ равно конкретному значению (0 или 1) при этом, если $\Pr[y_i = 1 \, | \, M = m, x = i] = p$, то рассмотрим 2 случая: \begin{enumerate}
    \item Алиса решает, что $y_x = 1$, то она ошибется с вероятностью $ 1 - p$
    \item Алиса решает, что $y_x = 0$, то она ошибется с вероятностью $ p$
\end{enumerate}
То есть в обоих случаях распределения совпадают, а значит энтропия одинакова.

Чтобы алгоритм был корректен, $ \E_{i, m} (r_m^{i}) \le \frac{1}{2} - \delta $.

Теперь $ D(\pi) \ge \Omega ( \delta^2 n)$.
То есть нам нужно передать хотя бы $ \Omega(\delta ^2 n)$ бит. С другой стороны можем построить простой протокол с $ 2 \delta n$ битами. Для этого Боб передаст первые $ 2 \delta n$ бит, а затем Алиса, если нужный ей бит есть, выдаст его, иначе выдаст 0. Вероятность ошибки в таком случае ровно такая, как мы хотели: $ \frac{1}{2} (1 - 2\delta )$. Эта верхняя оценка больше нашей нижней. На самом деле она улучшается.

% на лекции было \log \lvert M \rvert \ge D(\pi) \ldots 

\dotfill

\begin{defn}
Пусть $  \mu$ --- вероятностная мера на $ X \times Y$. 

$ \IC_{\mu}^{ext}(\pi) \coloneqq I( \pi(X, Y): (X, Y))$ --- внешнее информационное разглашение.

$ \IC_{\mu}^{int}(\pi) \coloneqq I( \pi(X, Y) : X \mid Y) + I( \pi(X, Y) : Y \mid X) $ -- внутреннее информационное разглашение.
\end{defn}

\textit{Интуиция}: 
\begin{enumerate}
    \item Внешнее информационное разглашение --- сколько информации получит внешний наблюдатель об $(X, Y)$, зная значение протокола $\pi(X, Y)$.
    \item Внутреннее информационное разглашение --- сколько информации получат внутренние  наблюдатели о входах друг друга, посмотрев значения протокола.
\end{enumerate}

\begin{thm}\label{thm:4_7_1}
	$ D( \pi) \ge  \IC_{\mu}^{ext}( \pi) \ge \IC_{ \mu}^{int}(\pi)$
\end{thm}
\begin{proof}
	\begin{itemize}
		\item Первое неравенство очевидно, так как 
    \[
        I(\pi(X, Y) : (X, Y)) \le H(\pi(X,Y)) \le \log |L| \le D(\pi(X, Y))
    \] 
    $ L$ --- листья протокола. 
    
		\item Докажем вторую часть теоремы. Для этого покажем $ I( \pi : x, y) \ge I( \pi : x \mid y) + I(\pi : y \mid x)$. 
    
    Если оставить только одно слагаемое, слева останется
    $ H( \pi ) - H( \pi \mid x, y)$
    , а справа 
    $ H( \pi \mid y ) - H( \pi \mid x, y)$, которое точно не больше.
    
    Пусть $  \pi_i$ -- $i $-ый бит $\pi$, то есть то, что Алиса и Боб отправили за $ i$-ый раунд.
    \begin{align*}
    	I( \pi : x, y) &= \tag{Chain rule} \sum_{i} I( \pi_i : x, y \mid \pi_{< i}) 
    \end{align*}
    Аналогично попробуем нарезать слагаемые правой части и оценим каждую часть по отдельности:
    
    \begin{align*}
    	I( \pi : x \mid y) & = \sum_{i}^{} I( \pi_i : x \mid y, \pi_{<i}) \\
    	I( \pi : y \mid x) & = \sum_{i}^{} I( \pi_i : y \mid x, \pi_{<i}) 
    \end{align*}
    
    В ход Боба первое слагаемое <<равно нулю>>, так как $  \pi_i$ определяется $ y $-ом. Аналогично второе <<равно нулю>>, когда ходит Алиса. Поэтому каждый раз неравенство сохраняется. На самом деле есть ошибка в рассуждениях, т.к. $\pi_{<i}$ с.в., то при разных значениях $\pi_{<i}$ ходить может либо Боб, либо Алиса, а не всегда кто-то один при всех значениях, как мы пытались обосновать.
    
    Чтобы исправить скрытую фундаментальную ошибку распишем через матожидания
    \begin{align*}
    	I( \pi : x, y) &= \sum_{i}^{} \E_{m}I( \pi_i : x, y \mid \pi_{< i} = m) \\
    	I( \pi : x \mid y) & = \sum_{i}^{} \E_m I( \pi_i : x \mid y, \pi_{<i} = m) \\
    	I( \pi : y \mid x) & = \sum_{i}^{} \E_mI( \pi_i : y \mid x, \pi_{<i} = m) 
    \end{align*}
    Это уже корректное утверждение, так как для каждого конкретного $m$ одно из слагаемых действительно обнуляется, значит для матожиданий неравенство есть, ну и значит для суммы тоже.
	\end{itemize}
\end{proof}


\begin{thm}[Храпченко]
	$ L( XOR) \ge \Omega (n^2)$
\end{thm}
\begin{proof}
           Покажем, что для любого протокола $ \pi $ задачи $ \KW_{\oplus_n} $ существует такое распределение $\mu$, что $ \IC^{int}_{\mu}(\pi) \ge 2\log n$. Отсюда будет следовать, что (пользуясь выводами в \hyperref[thm:4_7_1]{теореме \ref{thm:4_7_1}}) $ D(\pi) \ge \log{m} \ge \log{|L|} \ge 2\log n$, где $m$ -- кол-во вершин, $|L|$ - кол-во листьев, а значит и $L(\oplus_n) \ge n^2 $. 
       Пусть распределение $ \mu $ будет равномерным на всех парах вида $ (x,x\oplus e_i)$, где $\oplus_n(x) = 1$, то есть в $ x $ нечётное число единиц, а строка $e_i$ имеет единицу в позиции $i$ и нули во всех остальных. Таким образом, пары входов из распределения $\mu$ всегда будут отличаться только в одном бите. По определению,
    $$ \IC^{int}_{\mu}(\pi) = I(\pi : X \mid Y) + I(\pi : Y \mid X). $$
    
    Рассмотрим одно из слагаемых $I(\pi : Y \mid X) $. Имеем:
    $$
    I(\pi : Y \mid X) = I(\pi : X \oplus e_i \mid X) = H(X \oplus e_i \mid X) - \underbrace{H(X\oplus e_i \mid X, \pi)}_{=0} = H(e_i) = \log n
    $$
    Аналогичное равенство верно и для $ I(\pi \colon Y\mid X) $. Таким образом, $\IC^{int}_{\mu}(\pi) \geq 2\log n $, что и требовалось. Верхнюю оценку на $ D(\pi) $ нетрудно доказать, явно построив функцию.
\end{proof}
\vspace{1em}
