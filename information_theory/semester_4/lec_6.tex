\section{Теория информации в коммуникационной сложности}
\lecture{6}{6 May}{\dag}
\subsection{Подсчет функции индексов}
\begin{defn}
Определим \selectedFont{функцию индексирования} следующим образом:
 \[
	\Ind \colon [n]\times \{0, 1\}^{n} \to \{0, 1\}, \qquad \Ind(x, y) = y_x
.\] 
\end{defn}
Алиса и Боб хотят посчитать эту величину, причем $ x$ у Алисы, а  $ y $ у Боба.

Пусть сообщения идут только от Боба до Алисы. Несложно понять, что Бобу придется послать всю информацию.

Теперь предположим, что они хотят, чтобы Алиса посчитала $ \Ind$  верно с вероятностью $ \frac{1}{2} + \delta $, если $ x$ и $ y$ выбираются равномерно. Очевидно, для $  \delta  = 0$, просто ничего не нужно пересылать. А вот для других положительных значений все испортится.


\dotfill


Пусть $ M(y)$ --- случайная величина. Сообщение, которое передает Боб Алисе в зависимости от своего входа.
\begin{align*}
	I(M : y)& = \tag{Chain rule} \\
			&= \sum_{i} I(M: y_i \mid y_{<i}) = \\
			&= \sum_i H(y_i \mid y_{<i}) - H(y_i \mid M, y_{<i}) = \tag{$ y_i$ независимы} \\
			&= \sum_i H(y_i) - H(y_i \mid M, y_{<i}) \ge \tag{Выкинули часть условий} \\
			& \ge \sum_i H(y_i) - H(y_i \mid M) = \\
			&= \sum I(M : y_i)
\end{align*}

Покажем, что полученная сумма большая.

Зафиксируем $ i$ и распишем по определению взаимной информации: 
\begin{align*}
	\sum_{i} I(M : y_i) &= \sum_{i} H(y_i) - H(y_i \mid M) = \tag{$ H(y_i) = 1$} \\
	           &= \sum_{i}  1 - \E _{m} \left( H(y_i \mid M= m \right) \tag{$y$ не зависит от $x$, $M$ определяется $y$} =\\
			   &= \sum_{i} 1 - \E _{m} \left( H(y_i \mid M= m, x= i \right)  = \\
			   &= \sum_{i} 1 - H(\E_{m}(r_m^{i}))   \\
			   &= n - n \sum_{i}^{} \frac{1}{n} H(\E_{m}(r_m^{i})) \ge \tag{Неравенство Йенсена}\\
			   &\ge n \cdot (1 - H(\E_{m, i} (r_m^{i}))) \ge \tag{Монотонность энтропии на $(0, \tfrac{1}{2})$} \\
			   & \ge  n \cdot  (1 - H(\tfrac{1}{2} - \delta )) = \tag{Ряд Тейлора для энтропии}\\
			   &= \Omega ( \delta ^2 n)
\end{align*}
Здесь $ r_m^{i} $ --- характеристическая функция ошибки  $ M=m$,  $ x = i$. 

\noindent\textit{Пояснение к переходу к  $ r_m^{i} $}:
Энтропии равны, так как распределения совпадают. Если $\Pr[y_i = 1 \, | \, M = m, x = i] = p$, то Алиса ошибется или не ошибется с вероятностью $p$ (так как алгоритм детерминированный).

Чтобы алгоритм был корректен, $ \E_{i, m} (r_m^{i}) \le \frac{1}{2} - \delta $.

Теперь $ \log \lvert M \rvert  \ge H(M) \ge \Omega ( \delta ^2 n)$. 
То есть нам нужно хотя бы такое количество бит передать. С другой стороны можем построить простой протокол с $2\delta n$ битами. Для этого Боб передаст первые $2\delta n$ бит, а затем Алиса, если нужный ей бит есть, выдаст его, иначе выдаст 0. Вероятность ошибки в таком случае ровно такая, как мы хотели: $	\frac{1}{2} (1 - 2\delta )  $. Эта верхняя оценка больше нашей нижней. На самом деле она улучшается.


\dotfill

\begin{defn}
Пусть $  \mu$ --- вероятностная мера на $ X \times Y$. 

$ \IC_{\mu}^{ext} \coloneqq I( \pi(X, Y): (X, Y))$ --- внешнее информационное разглашение.

$ \IC_{\mu}^{int} \coloneqq I( \pi(X, Y) : X \mid Y) + I( \pi(X, Y) : Y | X) $ -- внутреннее информационное разглашение.
\end{defn}

Интуиция определения следующая. Внешнее информационное разглашение есть количество информации, которое получит внешний наблюдатель об $(X, Y)$, зная значение протокола $\pi(X, Y)$. Внутреннее информационное разглашение --- сколько информации получат внутренние наблюдатели о входах друг друга, посмотрев значения протокола.

\begin{thm}\label{thm:4_7_1}
	$ D( \pi) \ge  \IC_{\mu}^{ext}( \pi) \ge \IC_{ \mu}^{int}(\pi)$
\end{thm}
\begin{proof}
    Первое неравенство очевидно, так как $I(\pi(X, Y) : (X, Y)) \le H(\pi(X,Y)) \le \log |M| \le D(\pi(X, Y))$. $M$ --- листья протокола. 
    Второе докажем \hyperref[proof:thm_4_7_1]{потом}. 
\end{proof}
\begin{thm}[Храпченко]
	$ L( XOR) \ge \Omega (n^2)$
\end{thm}
\begin{proof}
       Покажем, что для любого протокола $ \pi $ задачи $ \KW_{\oplus_n} $ существует такое распределение $\mu$, что $ \IC^{int}_{\mu}(\pi) \ge 2\log n$. Отсюда будет следовать, что $ D(\pi) \ge 2\log n$ и $L(\oplus_n) \ge n^2 $. Распределение $ \mu $ будет равномерным на всех парах вида $ (x,x\oplus e_i)$, где $\oplus_n(x) = 1$, то есть в $ x $ нечётное число единиц, а строка $e_i$ имеет единицу в позиции $i$ и нули во всех остальных. Таким образом, пары входов из распределения $\mu$ всегда будут отличаться только в одном бите. По определению,
    $$ \IC^{int}_{\mu}(\pi) = I(\pi : X \mid Y) + I(\pi : Y \mid X). $$
    
    Рассмотрим одно из слагаемых $I(\pi : Y \mid X) $. Имеем:
    $$
    I(\pi : Y \mid X) = I(\pi : X \oplus e_i \mid X) = H(X \oplus e_i \mid X) - H(X\oplus e_i \mid X, \pi) = H(e_i) = \log n
    $$
    Аналогичное равенство верно и для $ I(\pi \colon Y\mid X) $. Таким образом, $\IC^{int}_{\mu}(\pi) \geq 2\log n $, что и требовалось. Верхнюю оценку на $ D(\pi) $ нетрудно доказать, явно построив функцию.
\end{proof}
\vspace{1em}
