\lecture{2}{8 April}{\dag}
\section{Задача выполнимости}
\textbf{Вход:} $ \Phi = \wedge C_i $  --- формула в КНФ.

Будем подставлять значения во все переменные по очереди, тем самым перемещаться по двоичному дереву.

Подставим $ x_i = 0$. Если пока клозы не нарушены, подставляем далее $ x_{i+1} = 0$ и проверяем клозы аналогично. Если же один из клозов нарушился, вернемся на шаг назад и подставим $  x_i = 1$ .

\begin{figure}[ht]
    \centering
    \incfig{cnf-algo}
    \label{fig:cnf-algo}
\end{figure}

Это достаточно эффективный алгоритм, причем мы не ограничиваем выбор последовательности подстановок, порядок $ 0$ и $ 1$. Таким образом, если есть выполняющий набор, и мы на первом шаге подберем верное значение первой переменной, на второй для второй и т.д., то получим оптимальное решение. Однако не всегда такой алгоритм работает быстро. Пример: задача про рассадку голубей.

\section{Раcсадка голубей}
У нас есть $n+1$ голубь и $n$ клеток, хотим показать, что нельзя рассадить в клетку по одному голубю.

Введем для каждой пары (голубь, клетка) переменную $ x_{ij}$, которая равна $ 1$, если $ i$-ый голубь сидит в $ j$-ой клетке, и $ x_{ij} = 0$ иначе.

Тогда, чтобы рассадка была удачной, должны выполняться следующие условие: 
\begin{itemize}
	\item для всех $i \in [n+1]$ верно $ \prod_{j=1}^{n} (1-x_{ij}) = 0$. То есть для каждого голубя нашлась клетка.
	\item для всех $ i, i'\in [n+1]$ и $ j \in [n]$, где  $ i \ne i'$ верно $x_{ij} \cdot x_{i'j} = 0$. То есть никакие два голубя не сидят в одной клетке.
\end{itemize}
Пусть один игрок загадал расстановку голубей, а второй хочет найти дизъюнкт, для которого нарушается эта расстановка. 
Игра с монетками позволяет показать, что предыдущий алгоритм плохо работает на этой модели.

\chapter{Информация по Шеннону}
\section{Определения и свойства}
На прошлой лекции поняли, что не всегда можем отличить некоторые множества.

Попробуем исправить данную ситуацию. Хотим понять состояния в $ Y$, зная информацию об $ X$. В среднем нам нужно сильно меньше информации, чем в крайнем случае.

Введем новую меру информации $  \mu ( \alpha )$, где $  \alpha $ --- распределение (множество и вероятности каждого элемента). Причем хотим, чтобы основные свойства были согласованы: \footnote{$ \mu(x, y) = \mu((x, y))$}
\begin{enumerate}
	\item $  \mu(U_n) = \log n$;
	\item $ \mu( \alpha ) \ge 0$;
	\item $\mu ( \alpha,  \beta ) = \mu ( \alpha ) + \mu ( \beta )$, если  $  \alpha $ и $  \beta $ независимы.\footnote{$( \alpha,  \beta )$ --- распределение на парах.}
\end{enumerate} 
Если действовать как настоящие математики, можно переписать эти свойства в более общие:
\begin{enumerate}
	\item \textit{монотонность}: $ \mu(U_{M}) \ge \mu(U_{M'})$, если $ \lvert M \rvert \ge \lvert M' \rvert $ ;
	\item \textit{аддитивность}: $\mu ( \alpha,  \beta ) = \mu ( \alpha ) + \mu ( \beta )$, если  $  \alpha $ и $  \beta $ независимы;
	\item \textit{непрерывность}: $ \mu(B_p)$ непрерывно по $ p \in [0, 1]$, где $ B_p  $ --- распределение Бернулли для $ p$. 
	\item \textit{согласованность с условной вероятностью}: 
		$$ \mu(B_p, \alpha ) = \mu(B_p) + \Pr[B_p = 0]\cdot \mu( \alpha \mid B_p = 0) + \Pr[B_p = 1]\cdot \mu( \alpha \mid B_p = 1).$$
\end{enumerate} 
Этим аксиомам удовлетворяет примерно одна функция $  \mu(X) \coloneqq \sum p_i \log \frac{1}{ p_i} $ с точностью до домножения на константу.

\subsection{Энтропия}
\begin{defn}[Энтропия]\index{энтропия}
	Для случайной величины $  \alpha $ с вероятностями событий $ (p_1, p_2, \ldots )$ меру
\[
	H( \alpha ) = \sum_{i = 1}^{\lvert \supp ( \alpha ) \rvert } p_i \log \frac{1}{p_i}
\] 
	будем называть \selectedFont{энтропия} и обозначать $ H$.\footnote{$ \supp \alpha $ --- все возможные события, то есть имеющие ненулевую вероятность}

Энтропия обозначает среднее по распределению $  \alpha $ необходимое количество информации для записи элемента.
\end{defn}
\begin{note}
    Энтропия равномерного распределения $U_n$ равна $ \log n$, так как вероятность каждого события $ \frac{1}{n}$.
\end{note}
\begin{note}
	Далее $ H(p) $ обозначает энтропию для распределения нечестной монетки.
	\[
		H(B_p) = H(p) = p \log \frac{1}{p} + (1-p) \log \frac{1}{1-p}
	.\] 
\end{note}

\begin{thm}
	$ H( \alpha ) \le  \log \lvert \supp ( \alpha ) \rvert $
\end{thm}
\begin{proof}
	Применим неравенство Йенсена
	 \[
	\begin{aligned}
		\sum_{i = 1}^{\lvert \supp ( \alpha ) \rvert } p_i \log \frac{1}{p_i} \le  \log \left( \sum_{i}^{} p_i \frac{1}{p_i} \right)  = \log \lvert \supp ( \alpha ) \rvert 
	\end{aligned}
	\]
\end{proof}
\begin{thm}
	$ H( \alpha , \beta ) \le H( \alpha ) + H ( \beta )$
\end{thm}
\begin{proof}
    \[
    \begin{aligned}
		H( \alpha , \beta ) &= \sum_{i, j}^{} p_{i, j} \log \frac{1}{p_{i, j}} \\
		H( \alpha ) + H( \beta ) &= \sum_{i}^{} p_i\log  \frac{1}{p_i} + \sum_{j}^{} p_j \log \frac{1}{p_j}
	\end{aligned}
	\]
	Заметим, что $ p_i = \sum\limits_{j}^{} p_{i, j}$ и $ p_j = \sum\limits_{i}^{} p_{i, j}$.
	\[
		H( \alpha , \beta ) -H( \alpha ) - H( \beta )= \sum_{i, j}^{} p_{i, j} \log \frac{1}{p_{i, j}}
		- \sum_{i}^{} p_i\log  \frac{1}{p_i} - \sum_{j}^{} p_j \log \frac{1}{p_j} = \sum_{i, j}^{ } p_{i, j} \log \frac{p_i p_j}{p_{i, j}}
	.\] 
	Если $  \alpha $ и $ \beta $ независимы, то все логарифмы обнуляются. Иначе по неравенству Йенсена
	\[
		\sum_{i, j}^{ } p_{i, j} \log \frac{p_i p_j}{p_{i, j}} \le  \log \left( \sum_{i, j}^{ } p_i p_j \right)  = 0
	.\] 
\end{proof}

\subsection{Условная энтропия}
\begin{defn}[Условная энтропия]
	\selectedFont{Энтропией $  \alpha $ при $ \beta = b$} будем называть энтропию распределения $  \alpha $ при условии, что $ \beta = b$, то есть:
	\[
	H( \alpha \mid \beta  = b) \coloneqq  \sum_{i}^{} \Pr[ \alpha = i \mid  \beta  = b] \cdot  \log \frac{1}{\Pr [ \alpha  = i \mid \beta  = b]}
.\]
Тогда \selectedFont{энтропией $  \alpha $ при условии $ \beta$} назовем среднее значение по $ \beta$ энтропии $  \alpha $ при $ \beta = b$:
\[
	H( \alpha  \mid \beta ) \coloneqq  \E_{\beta \to b } H( \alpha  \mid \beta = b)  = \sum_{b} H ( \alpha  \mid \beta = b) \cdot  \Pr[\beta = b]
.\] 
\end{defn}
\begin{prop}
    ~\begin{enumerate}
		\item $ H( \alpha \mid \beta ) \ge  0$
		\item $ H( \alpha  \mid \beta ) = 0 \Longleftrightarrow \alpha  \text{ однозначно определяется } \beta $ 
		\begin{proof*}
		    \[ 
		        H( \alpha \mid \beta ) = 0 
		        \Longleftrightarrow 
		        \forall b ~H(\alpha \mid \beta = b) = 0 
		        \Longleftrightarrow 
		        \forall b: ~\Pr[\alpha = i \mid \beta = b] = 1
		    \]
	    \end{proof*}
		\item $ \forall f \colon ~ H( \alpha  \mid \beta  ) \ge  H( f( \alpha ) \mid \beta )$
		\begin{proof*} 
		    Заметим, что достаточно показать неравенство $ H(\alpha \mid \beta = b) \ge H(f(\alpha) \mid \beta = b)$, поэтому будем считать, что все дальнейшие рассуждения проводятся при условии $ \beta = b$. \\
		    Пусть $ \alpha$ принимает значения $ a_1, \ldots, a_n$ с вероятностями $ p_1, \ldots, p_n$, а $ f(\alpha)$ принимает значения $ f_1, \ldots, f_k$ так, что для $ \forall j ~f_i := f(a_{i_j})$ (разбили $ a_i$-ые на $ k$ частей). Обозначим $ P_i := \sum_j p_{i_j}$ - вероятность $ f_i$. 
		    \begin{align*}
		        H(\alpha \mid \beta = b) &= 
		        \sum_i \Pr[\alpha = i \mid \beta = b] \cdot \log \frac{1}{\Pr[\alpha = i \mid \beta = b]}
		        \tag{по определению $ H$} \\
		        &= \sum_i p_i \cdot \log \frac{1}{p_i} 
		        \tag{по определению $ p_i$} \\
		        &= \sum_i \left( \sum_j p_{i_j} \cdot \log \frac{1}{p_{i_j}} \right) 
		        \tag{перегруппировали слагаемые}\\
		        &\ge \sum_i \left( \log \frac{1}{P_i} \cdot \sum_j p_{i_j} \right) \tag{оценили максимум: $ P_i \ge p_{i_j}$}\\
		        &= \sum_i P_i \cdot \log \frac{1}{P_i}
		        \tag{свернули сумму в $ P_i$} \\
		        &= H(f(\alpha) \mid \beta = b) 
		    \end{align*} 
		\end{proof*}
		\item $ H( \alpha , \beta ) = H( \alpha ) + H( \beta  \mid \alpha ) = H(  \beta ) + H( \alpha \mid \beta )$
		\begin{proof*}
		    \[
    		    H( \beta ) + H( \alpha  \mid \beta ) = \sum_j p_j \log \frac{1}{p_j} + \sum_{j} H ( \alpha  \mid \beta = j) \cdot  \Pr[\beta = j] = \]\[ \sum_j p_j \log \frac{1}{p_j} + \sum_{j} \left(\sum_{i}^{} \Pr[ \alpha = i \mid  \beta  = j] \cdot  \log \frac{1}{\Pr [ \alpha  = i \mid \beta  = j]}\right) \cdot  \Pr[\beta = j] = 
		    \]
		    \[ 
		        \sum_j p_j \log \frac{1}{p_j} + \sum_{j} \left(\sum_{i}^{} \Pr[ \alpha = i,  \beta  = j] \cdot  \log \frac{\Pr [\beta = j]}{\Pr [ \alpha  = i,  \beta  = j]}\right) = 
		    \]
		     \[ 
		         \sum_j \sum_i p_{i,j} \log \frac{1}{p_j} + \sum_{j} \left(\sum_{i}^{} p_{i,j} \cdot  \log \frac{p_j}{p_{i,j}}\right) = \sum_{i,j}p_{i,j}\log \frac{1}{p_{i,j}} = H(\alpha, \beta)
		    \]
		    подсказка: $\Pr [A, B] = \Pr[A \mid B] \cdot \Pr[B]$
		\end{proof*}
		\item $ H( \alpha , \beta ) \ge H( \alpha )$
		\begin{proof*}
		    Очевидно из предыдущего свойства.
		\end{proof*}
		\item $ H( \alpha ) \ge  H ( \alpha \mid \beta )$
			\begin{proof*}
			    \begin{align*}
    			    H( \alpha \mid \beta ) - H ( \alpha ) 
    			    &= \sum_j \sum_i\left( \Pr [\alpha = i \mid \beta = j] \log \frac{1}{ \Pr [ \alpha = i \mid \beta = j]}\right)\cdot \Pr[\beta = j] - \sum p_{i} \log \frac{1}{p_i} \\
    				&= \sum p_{i,j}\log\frac{p_j}{p_{i,j}} - \sum p_{i,j}\log\frac{1}{p_i} \\
    				&= \sum p_{i, j} \log \frac{p_i p_i}{p_{i,j}} \\
    				&\le \log \sum p_i p_j = \log 1 = 0 \tag{по нер-ву Йенсена}
			    \end{align*}
			\end{proof*}
		\item Формула условной энтропии через отдельные вероятности
		    \begin{proof*}
    			\begin{align*}
    				H( \alpha \mid \beta ) 
    				&= \sum_j H(\alpha \mid \beta = b_j) \cdot \Pr[\beta = b_j] \\
    				&= \sum_{j} \Pr[ \beta = b_j] \cdot  \sum_{i}^{} \Pr[ \alpha = a_i \mid \beta = b_j] \cdot \log \frac{1}{\Pr[ \alpha = a_i \mid \beta = b_j]} \\
    				&= \sum_{i, j}^{} p_{i,j} \cdot \log \frac{p_{j}}{p_{i, j}}
    			\end{align*}
			\end{proof*}
		\item $ H( \alpha \mid \beta) \ge H( \alpha  \mid \beta, \gamma ) $
		    \begin{proof*}
		        \begin{align*}
		            H(\alpha \mid \beta) &= H(\alpha, \beta) - H(\beta)\\
		            H(\alpha \mid \beta, \gamma) &= H(\alpha, \beta, \gamma) - H(\beta, \gamma)\\
		            H(\alpha \mid \beta) - H(\alpha \mid \beta, \gamma) &= H(\alpha, \beta) - H(\beta) - \Big(H(\alpha, \beta, \gamma) - H(\beta, \gamma)\Big) \\
		            &=  H(\beta, \gamma) - H(\beta) - \Big(H(\alpha, \beta, \gamma) - H(\alpha, \beta)\Big) \\
		            &= H(\gamma \mid \beta) - H(\gamma \mid \alpha, \beta)
		        \end{align*}
		    \end{proof*}
		\item 
			$ 2H( \alpha , \beta , \gamma ) \le H( \alpha , \beta ) + H( \alpha,  \gamma ) + H( \beta , \gamma ).$
		\begin{proof*}
        \\
		 Заметим, что $H(\alpha, \beta, \gamma) = H(\alpha) + H(\beta, \gamma \mid \alpha) = H(\alpha) + H(\beta \mid \alpha) + H(\gamma \mid \alpha, \beta)$.\\
		 Теперь распишем аналогичное для правой части и воспользуемся $H(\alpha) \ge H(\alpha \mid \beta)$:
		 \[
		    H(\alpha, \beta) = H(\alpha) + H(\beta \mid \alpha)
		 \]
		 \[
		    H(\alpha, \gamma) = H(\alpha) + H(\gamma \mid \alpha) \ge H(\alpha) + H(\gamma \mid \alpha, \beta)
		 \]
		 \[
		    H(\beta, \gamma) = H(\beta) + H(\gamma \mid \beta) \ge H(\beta \mid \alpha) + H(\gamma \mid \alpha, \beta)
		 \]
		 Теперь если сложить все три формулы, то получим то, что надо.
		\end{proof*}
		
    \end{enumerate} 
\end{prop}

\section{Взаимная информация}
\begin{defn}
	\selectedFont{Взаимная информация} между случайными величинами $  \alpha $ и $ \beta $:
	\[
		I( \alpha  : \beta ) \coloneqq H( \alpha ) - H( \alpha \mid \beta )
	.\] 
	\selectedFont{Взаимная информация в $  \alpha $ и $ \beta$ при условии $ \gamma $ }:
		\[
			I ( \alpha : \beta \mid \gamma ) \coloneqq  H( \alpha  \mid \gamma ) - H( \alpha  \mid \beta , \gamma )
		.\] 
\end{defn}
\begin{prop}
	~\begin{enumerate}
		\item $ I( \alpha : \beta ) = I( \beta : \alpha )$.
			\begin{proof*}
				$ I( \alpha : \beta ) = H( \alpha ) - H( \alpha \mid \beta ) = H( \alpha ) - H( \alpha , \beta ) + H( \beta ) = H( \beta ) - H( \beta \mid \alpha ) = I( \beta : \alpha )$
			\end{proof*}

		\item $  \alpha $ и $  \beta $ независимы тогда и только тогда, когда $ I( \alpha : \beta ) = 0$.
			\begin{proof*}
				$ I( \alpha : \beta ) = 0 \Longleftrightarrow H( \alpha ) = H( \alpha \mid \beta ) \Longleftrightarrow $ независимы $  \alpha $ и $ \beta $.
			\end{proof*}
		\item $ I( f( \alpha ) \colon \beta ) \le I ( \alpha : \beta )$ для любой функции $ f$.
		\begin{proof*}
		    Для начала перепишем взаимную информацию:
		    \begin{align*}
		        I(\alpha : \beta) &= H(\alpha) - H(\alpha \mid \beta) \\
		        &= \sum_{i,j}p_{i,j}\log\frac{1}{p_i} - \sum_{i,j}p_{i,j}\log\frac{p_j}{p_{i,j}} \\
		        &= \sum_{i,j} p_{i,j}\log\frac{p_{i,j}}{p_i p_j} = \sum_{k,j}\sum_{i\in f^{-1}(k)}p_{i,j}\log\frac{p_{i,j}}{p_i p_j}
		        \tag{сгруппировали с помощью прообразов}
		    \end{align*}
		    Аналогично
		    \begin{align*}
		        I(f(\alpha) : \beta) &= \sum_{k, j}\Pr[f(\alpha) = k, \beta = j]\log\frac{\Pr[f(\alpha) = k, \beta = j]}{\Pr[f(\alpha) = k]\Pr[\beta = j]}\\
		        &= \sum_{k,j}\left(\sum_{i \in f^{-1}(k)}p_{i,j}\right)\log\frac{\left(\sum_{i \in f^{-1}(k)}p_{i,j}\right)}{\left(\sum_{i \in f^{-1}(k)}p_{i}\right)p_j}
		    \end{align*}
		    Теперь для каждой пары $k,j$ покажем неравенство отдельно
		    \begin{align*}
		        &-\sum_{i\in f^{-1}(k)}p_{i,j}\log\frac{p_{i,j}}{p_i p_j} = \sum_{i\in f^{-1}(k)}p_{i,j}\log\frac{p_i p_j}{p_{i,j}}\\
		        &=\left(\sum_{c \in f^{-1}(k)}p_{c,j}\right)\sum_{i\in f^{-1}(k)}\frac{p_{i,j}}{\left(\sum_{c\in f^{-1}(k)}p_{c,j}\right)}\log\frac{p_i p_j}{p_{i,j}}
		        \tag{поделили и домножили для того чтобы сумма коэффициентов была 1}\\
		        &\le \left(\sum_{c\in f^{-1}(k)}p_{c,j}\right)\log\left(\sum_{i\in f^{-1}(k)}\frac{p_{i}p_j}{\left(\sum_{c\in f^{-1}(k)}p_{c,j}\right)}\right)
		        \tag{по нер-ву Йенсена}\\
		        &=\left(\sum_{i\in f^{-1}(k)}p_{c,j}\right)\log\frac{(\sum_{i\in f^{-1}(k)} p_{i})p_j}{\left(\sum_{i\in f^{-1}(k)}p_{i,j}\right)} = -\left(\sum_{i\in f^{-1}(k)}p_{i,j}\right)\log\frac{\left(\sum_{i\in f^{-1}(k)}p_{i,j}\right)}{(\sum_{i\in f^{-1}(k)} p_{i})p_j}
		    \end{align*}
		\end{proof*}
		\item  Следующие формулы для взаимной информации эквивалентны
			\begin{align*}
				I( \alpha : \beta ) &= H( \alpha) - H( \alpha  \mid \beta  ) \\
										 &= H( \beta ) - H( \beta \mid \alpha ) \\
										 &= H\left( \alpha  \right) + H( \beta ) - H( \alpha , \beta ) \\
										 &= H( \alpha , \beta ) - H( \alpha \mid \beta ) - H( \beta \mid \alpha )
			\end{align*}
	\end{enumerate}
\end{prop}

\section{Применение энтропии}
\subsection{Взвешивания монеток}
Попробуем решить задачу с монетками. Мы взвешиваем $ 14$ монеток и хотим найти фальшивую за три взвешивания, причем неизвестен относительный вес. В нашем графе есть только один исход со всеми равенствами. Докажем, что нет такой стратегии.

Пусть нам дана такая стратегия первого игрока.
Тогда мы получаем дерево с $ 27$ листьями, где во всех листьях, кроме одного, записана пара:
номер фальшивой монетки и ее относительный вес.
В единственной ветке, где все взвешивания давали равенство, будет записан только номер фальшивой монетки.
Построим равномерное распределение на этих исходах.

При равномерном распределении энтропия $ \log 27$.

Если стратегия верная, то
\begin{align*}
\log 27 = H(\alpha) &\le  H( \alpha, q_1, q_2, q_3) \le \\ &\le H( q_1 ) + H(q_{2} \mid q_1 )+ H(q_3 \mid q_1, q_2)+ H( \alpha  \mid q_1, q_2, q_3 ) \le \tag{Chain rule}\\  
			& \le  H(q_1) + H(q_2) + H(q_3) + 0
\end{align*}

Так как $ H(q_i) \le  \log 3$, для всех $ i$ выполнено равенство.

Чтобы было так, мы должны в каждый ход равновероятно получать все три ответа.
Пусть мы взвешиваем кучки из $ k$ монет\footnote{Очевидно, что если взвешивать кучки разного размера, информацию извлечь не получиться даже по Хартли}. 
Вероятность того, что левая кучка будет легче в результате первого взвешивания $\frac{2k}{27}$ (так как у нас либо в левой кучке фальшивая, и она легче, либо в правой кучке фальшивая, и она тяжелее). Тогда $\frac{2k}{27} = \frac{1}{3}$. $k$ получилось нецелым.
Противоречие.

\subsection{Оценка на биноминальные коэффициенты}
\[
	\sum_{i=0}^{k} {n \choose i} \le  2^{n H(\frac{k}{n})}
.\] 
Обозначим сумму за $ C$.

Будем выбирать множество размера не больше $ k$ равновероятно, а затем проверять, попало ли  $ i$ наше множество.
Пусть $ X_i$ --- индикатор того, что $ i$ выбрали. 
\begin{align*}
	\log C = H(X) &\le  H( X_1, \ldots , X_n) \le \\ \tag{Chain rule}
				  & \le \sum_{ }^{ } H( X_i \mid X_{<i}) \le \\
				  & \le \sum_{  }^{ } H(X_i) =  n H(X_1) \le  \tag{считаем, что $ k \le \frac{n}{2}$} \\ 
				  & \le n H\left( \tfrac{k}{n} \right)  
\end{align*}
\textit{Пояснение к последнему переходу.} $H(\frac{k}{n}) = \frac{k}{n} \log \frac{n}{k} + (1 - \frac{k}{n}) \log \frac{1}{1 - \frac{k}{n}}$. С другой стороны $H(X_1) = p \log \frac{1}{p} + (1-p) \log \frac{1}{1-p}$, где $p$~--- вероятность того, что первый элемент попал в множество. Заметим, что функция $f(x) = x \log \frac{1}{x} + (1 - x) \log \frac{1}{1-x}$ возрастает на $(0, \frac{1}{2})$. То есть нужно лишь показать, что $p \le \frac{k}{n}$. Для этого посчитаем матожидание числа элементов, которые вошли в выбранное множество.
\begin{align*}
    k \ge \E(N) = \E(\sum X_i) = \sum \E(X_i) = \sum p_i
\end{align*}
Но каждый элемент войдет в множество равновероятно, тогда $p \le \frac{k}{n}$.

