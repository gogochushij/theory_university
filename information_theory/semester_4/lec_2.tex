\lecture{2}{8 April}{\dag}
\subsection{Задача выполнимости}
Мы хотим решать задачу выполнимости.

\textbf{Вход:} $ \Phi = \wedge C_i $  --- формула в КНФ.

Подставим $ x_i = 0$. Если один из клозов нарушился, вернемся на шаг назад и подставим $  x_j = 1$, а иначе подставляем дальше.

\begin{figure}[ht]
    \centering
    \incfig{cnf-algo}
    \label{fig:cnf-algo}
\end{figure}

Это достаточно эффективный алгоритм, причем мы не ограничиваем выбор последовательности подстановок, порядок $ 0$ и $ 1$. Таким образом, если есть выполняющий набор, и мы на первом шаге подберем верное значение первой переменной, на второй для второй и т.д., то получим оптимальное решение. Однако не всегда такой алгоритм работает быстро. Пример: задача про рассадку голубей.

\section{Раcсадка голубей}
\begin{figure}[ht]
    \centering
    \incfig{golub-img}
    \label{fig:golub-img}
\end{figure}
У нас есть $n+1$ голубь и $n$ клеток, хотим показать, что нельзя рассадить в клетку по одному голубю.\\
$x_{ij}$ --- сажаем ли мы голубя $i$ в клетку $j$?
Тогда, чтобы рассадка была удачной, должны выполняться следующие условие: 
\begin{itemize}
    \item $\bigvee\limits_{j} x_{ij}$ для $i \in \{1, \dots, n+1\}$. То есть для каждого голубя нашлась клетка.
    \item $\neg x_{ik} \vee \neg x_{ij}$ для $k, j \in \{1, \dots , n \}$, $k \neq j$, $i \in \{1, \dots, n+1\}$. То есть никакие два голубя не сидят в одной клетке.
\end{itemize}
Пусть один игрок загадал расстановку голубей, а второй хочет найти дизъюнкт, для которого нарушается эта расстановка. Игра с монетками позволяет показать, что предыдущий алгоритм плохо работает на этой модели.


\chapter{Информация по Шеннону}
На прошлой лекции поняли, что не всегда можем отличить некоторые множества.

Попробуем исправить данную ситуацию. Хотим понять состояния в $ Y$, зная информацию об $ X$. В среднем нам нужно сильно меньше информации, чем в крайнем случае.

Введем новую меру информации $  \mu ( \alpha )$, где $  \alpha $ --- распределение (множество и вероятности каждого элемента). Причем хотим, чтобы основные свойства были согласованы: \footnote{$ \mu(x, y) = \mu((x, y))$}
\begin{enumerate}
	\item $  \mu(U_n) = \log n$;
	\item $ \mu( \alpha ) \ge 0$;
	\item $\mu ( \alpha,  \beta ) = \mu ( \alpha ) + \mu ( \beta )$, если  $  \alpha $ и $  \beta $ независимы. $( \alpha,  \beta )$ --- распределение на парах.
\end{enumerate} 

Если действовать как настоящие математики, можно переписать эти свойства в более общие:
\begin{enumerate}
	\item $ \mu(U_{M}) \ge \mu(U_{M'})$, если $ \lvert M \rvert \ge \lvert M' \rvert $ ;
	\item $\mu ( \alpha,  \beta ) = \mu ( \alpha ) + \mu ( \beta )$, если  $  \alpha $ и $  \beta $ независимы;
	\item $ \mu(B_p)$ непрерывно по $ p \in [0, 1]$, где $ B_p  $ --- распределение для монетки, вероятность орла -- $ p$. 
	\item $ \mu(B_p, \alpha ) = \mu(B_p) + Pr[B_p = 0]\cdot \mu( \alpha \mid B_p = 0) + Pr[B_p = 1]\cdot \mu( \alpha \mid B_p = 1)$.
\end{enumerate} 

\begin{defn}[Энтропия]\index{энтропия}
	Этим аксиомам удовлетворяет примерно одна функция $  \mu( \alpha ) \coloneqq k \cdot H( \alpha )$, где $ H( \alpha )$ --- \selectedFont{энтропия}.\footnote{$ \supp \alpha $ --- все возможные события, то есть имеющие ненулевую вероятность}
\[
	H( \alpha ) = \sum_{i = 1}^{\lvert \supp ( \alpha ) \rvert } p_i \log \frac{1}{p_i}
.\] 
Энтропия обозначает среднее по распределению $  \alpha $ необходимое количество информации для записи элемента.
\end{defn}
\begin{note}
    Энтропия равномерного распределения $U_n$ равна $ \log n$.
\end{note}
\begin{note}
	Далее $ H(p) $ обозначает энтропию для распределения монетки.
\end{note}

\begin{thm}
	$ H( \alpha ) \le  \log \lvert \supp ( \alpha ) \rvert $
\end{thm}
\begin{proof}
	Применим неравенство Йенсена
	 \[
	\begin{aligned}
		\sum_{i = 1}^{\lvert \supp ( \alpha ) \rvert } p_i \log \frac{1}{p_i} \le  \log \left( \sum_{i}^{} p_i \frac{1}{p_i} \right)  = \log \lvert \supp ( \alpha ) \rvert 
	\end{aligned}
	\]
\end{proof}
\begin{thm}
	$ H( \alpha , \beta ) \le H( \alpha ) + H ( \beta )$
\end{thm}
\begin{proof}
    \[
    \begin{aligned}
		H( \alpha , \beta ) &= \sum_{i, j}^{} p_{i, j} \log \frac{1}{p_{i, j}} \\
		H( \alpha ) + H( \beta ) &= \sum_{i}^{} p_i\log  \frac{1}{p_i} + \sum_{j}^{} p_j \log \frac{1}{p_j}
	\end{aligned}
	\]
	Заметим, что $ p_i = \sum\limits_{j}^{} p_{i, j}$ и $ p_j = \sum\limits_{i}^{} p_{i, j}$.
	\[
		H( \alpha , \beta ) -H( \alpha ) - H( \beta )= \sum_{i, j}^{} p_{i, j} \log \frac{1}{p_{i, j}}
		- \sum_{i}^{} p_i\log  \frac{1}{p_i} - \sum_{j}^{} p_j \log \frac{1}{p_j} = \sum_{i, j}^{ } p_{i, j} \log \frac{p_i p_j}{p_{i, j}}
	.\] 
	Если $  \alpha $ и $ \beta $ независимы, то все логарифмы обнуляются. Иначе по неравенству Йенсена
	\[
		\sum_{i, j}^{ } p_{i, j} \log \frac{p_i p_j}{p_{i, j}} \le  \log \left( \sum_{i, j}^{ } p_i p_j \right)  = 0
	.\] 
\end{proof}

\begin{defn}[Условная энтропия]
	\[
	H( \alpha \mid \beta  = b) = \sum_{i}^{} Pr[ \alpha = i \mid  \beta  = b] \cdot  \log \frac{1}{Pr [ \alpha  = i \mid \beta  = b]}
.\]
\[
	H( \alpha  \mid \beta ) = \E_{b = \beta } H( \alpha  \mid p = b)  = \sum_{b}^{ } H ( \alpha  \mid \beta = b) Pr[\beta = b]
.\] 
\end{defn}
\begin{prop}
    ~\begin{enumerate}
		\item $ \forall f \colon ~ H( \alpha  \mid \beta  ) \ge  H( f( \alpha ) \mid \beta )$
		\item $ H( \alpha , \beta ) = H( \alpha ) + H( \beta  \mid \alpha )$
		\item $ H( \alpha ) \ge  H ( \alpha \mid \beta )$
			\begin{proof}
			    \[
				H( \alpha \mid \beta ) - H ( \alpha ) = \sum_{ }^{ } p_{i, j} \frac{1}{\log Pr [ \alpha  = i \mid \beta  = j]} - \sum_{ }^{ } p_{i, j} \log \frac{1}{p_i} \le  \sum_{ }^{ } p_{i, j} \log \frac{p_i}{Pr [ \alpha = i \mid \beta  = j]} 
			    .\] 
				По неравенству Йенсена полученное выражение меньше нуля.
			\end{proof}
		\item $ H( \alpha \mid \beta) \ge H( \alpha  \mid \beta, \gamma ) $
    \end{enumerate} 
\end{prop}

\section{Применение энтропии}
\subsection{Взвешивания монеток}
Попробуем решить задачу с монетками. Мы взвешиваем $ 14$ монеток и хотим найти фальшивую за три взвешивания, причем неизвестен относительный вес. В нашем графе есть только один исход со всеми равенствами. Докажем, что нет такой стратегии.

Пусть нам дана такая стратегия первого игрока.
Тогда мы получаем дерево с $ 27$ листьями, где во всех листьях, кроме одного, записана пара:
номер фальшивой монетки и ее относительный вес.
В единственной ветке, где все взвешивания давали равенство, будет записан только номер фальшивой монетки.
Построим равномерное распределение на этих исходах.

При равномерном распределении энтропия $ \log 27$.

Если стратегия верная, то
\begin{align*}
\log 27 = H(\alpha) &\le  H( \alpha, q_1, q_2, q_3) \le \\ &\le H( q_1 ) + H(q_{2} \mid q_1 )+ H(q_3 \mid q_1, q_2)+ H( \alpha  \mid q_1, q_2, q_3 ) \le \tag{Chain rule}\\  
			& \le  H(q_1) + H(q_2) + H(q_3) + 0
\end{align*}

Так как $ H(q_i) \le  \log 3$, для все $ i$ выполнено равенство.

Чтобы было так, мы должны в каждый ход равновероятно получать все три ответа.
Пусть мы взвешиваем кучки из $ k$ монет\footnote{Очевидно, что взвешивать кучки разного размера, информацию извлечь не получиться даже по Хартли}. 
Вероятность того, что левая кучка будет легче в результате первого взвешивания $\frac{2k}{27}$ (так как у нас либо в левой кучке фальшивая, и она легче, либо в правой кучке фальшивая, и она тяжелее). Тогда $\frac{2k}{27} = \frac{1}{3}$. $k$ получилось нецелым.
Противоречие.

\subsection{Оценка на биноминальные коэффициенты}
\[
	\sum_{i=0}^{k} {n \choose i} \le  2^{n H(\frac{k}{n})}
.\] 
Обозначим сумму за $ C$.

Будем выбирать множество размера не больше $ k$ равновероятно, а затем проверять, попало ли  $ i$ наше множество.
Пусть $ X_i$ --- индикатор того, что $ i$ выбрали. 
\begin{align*}
	\log C = H(X) &\le  H( X_1, \ldots , X_n) \le \\ \tag{Chain rule}
				  & \le \sum_{ }^{ } H( X_i \mid X_{<i}) \le \\
				  & \le \sum_{  }^{ } H(X_i) =  n H(X_1) \le  \tag{считаем, что $ k \le \frac{n}{2}$} \\ 
				  & \le n H\left( \tfrac{k}{n} \right)  
\end{align*}
\textit{Пояснение к последнему переходу.} $H(\frac{k}{n}) = \frac{k}{n} \log \frac{n}{k} + (1 - \frac{k}{n}) \log \frac{1}{1 - \frac{k}{n}}$. С другой стороны $H(X_1) = p \log \frac{1}{p} + (1-p) \log \frac{1}{1-p}$, где $p$~--- вероятность того, что первый элемент попал в множество. Заметим, что функция $f(x) = x \log \frac{1}{x} + (1 - x) \log \frac{1}{1-x}$ возрастает на $(0, \frac{1}{2})$. То есть нужно лишь показать, что $p \le \frac{k}{n}$. Для этого посчитаем матожидание числа элементов, которые вошли в выбранное множество.
\begin{align*}
    k \ge \E(N) = \E(\sum X_i) = \sum \E(X_i) = \sum p_i
\end{align*}
Но каждый элемент войдет в множество равновероятно, тогда $p \le \frac{k}{n}$.

