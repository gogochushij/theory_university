\lecture{2}{8 April}{\dag}
\section{Задача выполнимости}
\textbf{Вход:} $ \Phi = \wedge C_i $  --- формула в КНФ.

Будем подставлять значения во все переменные по очереди, тем самым перемещаться по двоичному дереву.

Подставим $ x_i = 0$. Если пока клозы не нарушены, подставляем далее $ x_{i+1} = 0$ и проверяем клозы аналогично. Если же один из клозов нарушился, вернемся на шаг назад и подставим $  x_i = 1$ .

\begin{figure}[ht]
    \centering
    \incfig{cnf-algo}
    \label{fig:cnf-algo}
\end{figure}

Это достаточно эффективный алгоритм, причем мы не ограничиваем выбор последовательности подстановок, порядок $ 0$ и $ 1$. Таким образом, если есть выполняющий набор, и мы на первом шаге подберем верное значение первой переменной, на второй для второй и т.д., то получим оптимальное решение. Однако не всегда такой алгоритм работает быстро. Пример: задача про рассадку голубей.

\section{Раcсадка голубей}
У нас есть $n+1$ голубь и $n$ клеток, хотим показать, что нельзя рассадить в клетку по одному голубю.

Введем для каждой пары (голубь, клетка) переменную $ x_{ij}$, которая равна $ 1$, если $ i$-ый голубь сидит в $ j$-ой клетке, и $ x_{ij} = 0$ иначе.

Тогда, чтобы рассадка была удачной, должны выполняться следующие условие: 
\begin{itemize}
	\item для всех $i \in [n+1]$ верно $ \prod_{j=1}^{n} (1-x_{ij}) = 0$. То есть для каждого голубя нашлась клетка.
	\item для всех $ i, i'\in [n+1]$ и $ j \in [n]$, где  $ i \ne i'$ верно $x_{ij} \cdot x_{i'j} = 0$. То есть никакие два голубя не сидят в одной клетке.
\end{itemize}
Пусть один игрок загадал расстановку голубей, а второй хочет найти дизъюнкт, для которого нарушается эта расстановка. 
Игра с монетками позволяет показать, что предыдущий алгоритм плохо работает на этой модели.

\chapter{Информация по Шеннону}
\section{Определения и свойства}
На прошлой лекции поняли, что не всегда можем отличить некоторые множества.

Попробуем исправить данную ситуацию. Хотим понять состояния в $ Y$, зная информацию об $ X$. В среднем нам нужно сильно меньше информации, чем в крайнем случае.

Введем новую меру информации $  \mu ( \alpha )$, где $  \alpha $ --- распределение (множество и вероятности каждого элемента). Причем хотим, чтобы основные свойства были согласованы: \footnote{$ \mu(x, y) = \mu((x, y))$}
\begin{enumerate}
	\item $  \mu(U_n) = \log n$;
	\item $ \mu( \alpha ) \ge 0$;
	\item $\mu ( \alpha,  \beta ) = \mu ( \alpha ) + \mu ( \beta )$, если  $  \alpha $ и $  \beta $ независимы.\footnote{$( \alpha,  \beta )$ --- распределение на парах.}
\end{enumerate} 
Если действовать как настоящие математики, можно переписать эти свойства в более общие:
\begin{enumerate}
	\item \textit{монотонность}: $ \mu(U_{M}) \ge \mu(U_{M'})$, если $ \lvert M \rvert \ge \lvert M' \rvert $ ;
	\item \textit{аддитивность}: $\mu ( \alpha,  \beta ) = \mu ( \alpha ) + \mu ( \beta )$, если  $  \alpha $ и $  \beta $ независимы;
	\item \textit{непрерывность}: $ \mu(B_p)$ непрерывно по $ p \in [0, 1]$, где $ B_p  $ --- распределение Бернулли для $ p$. 
	\item \textit{согласованность с условной вероятностью}: 
		$$ \mu(B_p, \alpha ) = \mu(B_p) + \Pr[B_p = 0]\cdot \mu( \alpha \mid B_p = 0) + \Pr[B_p = 1]\cdot \mu( \alpha \mid B_p = 1).$$
\end{enumerate} 
Этим аксиомам удовлетворяет примерно одна функция $  \mu(X) \coloneqq \sum p_i \log \frac{1}{ p_i} $ с точностью до домножения на константу.

\subsection{Энтропия}
\begin{defn}[Энтропия]\index{энтропия}
	Для случайной величины $  \alpha $ с вероятностями событий $ (p_1, p_2, \ldots )$ меру
\[
	H( \alpha ) = \sum_{i = 1}^{\lvert \supp ( \alpha ) \rvert } p_i \log \frac{1}{p_i}
\] 
	будем называть \selectedFont{энтропия} и обозначать $ H$.\footnote{$ \supp \alpha $ --- все возможные события, то есть имеющие ненулевую вероятность}

Энтропия обозначает среднее по распределению $  \alpha $ необходимое количество информации для записи элемента.
\end{defn}
\begin{note}
    Энтропия равномерного распределения $U_n$ равна $ \log n$, так как вероятность каждого события $ \frac{1}{n}$.
\end{note}
\begin{note}
	Далее $ H(p) $ обозначает энтропию для распределения нечестной монетки.
	\[
		H(B_p) = H(p) = p \log \frac{1}{p} + (1-p) \log \frac{1}{1-p}
	.\] 
\end{note}

\begin{thm}
	$ H( \alpha ) \le  \log \lvert \supp ( \alpha ) \rvert $
\end{thm}
\begin{proof}
	Применим неравенство Йенсена
	 \[
	\begin{aligned}
		\sum_{i = 1}^{\lvert \supp ( \alpha ) \rvert } p_i \log \frac{1}{p_i} \le  \log \left( \sum_{i}^{} p_i \frac{1}{p_i} \right)  = \log \lvert \supp ( \alpha ) \rvert 
	\end{aligned}
	\]
\end{proof}
\begin{thm}
	$ H( \alpha , \beta ) \le H( \alpha ) + H ( \beta )$
\end{thm}
\begin{proof}
    \[
    \begin{aligned}
		H( \alpha , \beta ) &= \sum_{i, j}^{} p_{i, j} \log \frac{1}{p_{i, j}} \\
		H( \alpha ) + H( \beta ) &= \sum_{i}^{} p_i\log  \frac{1}{p_i} + \sum_{j}^{} p_j \log \frac{1}{p_j}
	\end{aligned}
	\]
	Заметим, что $ p_i = \sum\limits_{j}^{} p_{i, j}$ и $ p_j = \sum\limits_{i}^{} p_{i, j}$.
	\[
		H( \alpha , \beta ) -H( \alpha ) - H( \beta )= \sum_{i, j}^{} p_{i, j} \log \frac{1}{p_{i, j}}
		- \sum_{i}^{} p_i\log  \frac{1}{p_i} - \sum_{j}^{} p_j \log \frac{1}{p_j} = \sum_{i, j}^{ } p_{i, j} \log \frac{p_i p_j}{p_{i, j}}
	.\] 
	Если $  \alpha $ и $ \beta $ независимы, то все логарифмы обнуляются. Иначе по неравенству Йенсена
	\[
		\sum_{i, j}^{ } p_{i, j} \log \frac{p_i p_j}{p_{i, j}} \le  \log \left( \sum_{i, j}^{ } p_i p_j \right)  = 0
	.\] 
\end{proof}

\subsection{Условная энтропия}
\begin{defn}[Условная энтропия]
	\selectedFont{Энтропией $  \alpha $ при $ \beta = b$} будем называть энтропию распределения $  \alpha $ при условии, что $ \beta  = b$, то есть:
	\[
	H( \alpha \mid \beta  = b) \coloneqq  \sum_{i}^{} \Pr[ \alpha = i \mid  \beta  = b] \cdot  \log \frac{1}{\Pr [ \alpha  = i \mid \beta  = b]}
.\]
Тогда \selectedFont{энтропией $  \alpha $ при условии $ \beta$} назовем среднее значение по $ \beta$ энтропии $  \alpha $ при $ \beta = b$:
\[
	H( \alpha  \mid \beta ) \coloneqq  \E_{\beta \to b } H( \alpha  \mid p = b)  = \sum_{b} H ( \alpha  \mid \beta = b) \cdot  \Pr[\beta = b]
.\] 
\end{defn}
\begin{prop}
    ~\begin{enumerate}
		\item $ H( \alpha \mid \beta ) \ge  0$
		\item $ H( \alpha  \mid \beta ) = 0 \Longleftrightarrow \alpha  \text{ однозначно определяется } \beta $
		\item $ \forall f \colon ~ H( \alpha  \mid \beta  ) \ge  H( f( \alpha ) \mid \beta )$
		\item $ H( \alpha , \beta ) = H( \alpha ) + H( \beta  \mid \alpha ) = H(  \beta ) + H( \alpha \mid \beta )$
		\item $ H( \alpha , \beta ) \ge H( \alpha )$
		\item $ H( \alpha ) \ge  H ( \alpha \mid \beta )$
			\begin{proof}
			    \[
				H( \alpha \mid \beta ) - H ( \alpha ) = \sum_{ }^{ } p_{i, j} \frac{1}{\log \Pr [ \alpha  = i \mid \beta  = j]} - \sum_{ }^{ } p_{i, j} \log \frac{1}{p_i} \le  \sum_{ }^{ } p_{i, j} \log \frac{p_i}{\Pr [ \alpha = i \mid \beta  = j]} 
			    \] 
				По неравенству Йенсена полученное выражение меньше нуля.
			\end{proof}
		\item Формула условной энтропии через отдельные вероятности
			\begin{align*}
				H( \alpha \mid \beta ) &= \sum_{j} \Pr[ \beta = b_j] \cdot  \sum_{i}^{} \Pr[ \alpha = a_i, \beta = b_j] \cdot \log \frac{1}{\Pr[ \alpha = a_i, \beta = b_j]} \\
									   &= \sum_{i, j}^{} p_{i,j} \cdot \log \frac{p_{*, j}}{p_{i, j}}
			\end{align*}
		\item 
			$ 2H( \alpha , \beta , \gamma ) ≤ H( \alpha , \beta ) + H( \alpha,  \gamma ) + H( \beta , \gamma ).$
		\item $ H( \alpha \mid \beta) \ge H( \alpha  \mid \beta, \gamma ) $
			% \begin{proof*}
			% 	\begin{align*}
			% 		H( \alpha \mid \beta ) &= \sum_{b} H( \alpha \mid \beta  = b) \cdot \Pr[ \beta  = b] \\
			% 		H( \alpha \mid \beta, \gamma  ) &= \sum_{b,c} H( \alpha \mid \beta  = b, ~ \gamma = c) \cdot \Pr[ \beta  = b, ~ \gamma = c] 
			% 	\end{align*}
			% \end{proof*}
    \end{enumerate} 
\end{prop}

\section{Применение энтропии}
\subsection{Взвешивания монеток}
Попробуем решить задачу с монетками. Мы взвешиваем $ 14$ монеток и хотим найти фальшивую за три взвешивания, причем неизвестен относительный вес. В нашем графе есть только один исход со всеми равенствами. Докажем, что нет такой стратегии.

Пусть нам дана такая стратегия первого игрока.
Тогда мы получаем дерево с $ 27$ листьями, где во всех листьях, кроме одного, записана пара:
номер фальшивой монетки и ее относительный вес.
В единственной ветке, где все взвешивания давали равенство, будет записан только номер фальшивой монетки.
Построим равномерное распределение на этих исходах.

При равномерном распределении энтропия $ \log 27$.

Если стратегия верная, то
\begin{align*}
\log 27 = H(\alpha) &\le  H( \alpha, q_1, q_2, q_3) \le \\ &\le H( q_1 ) + H(q_{2} \mid q_1 )+ H(q_3 \mid q_1, q_2)+ H( \alpha  \mid q_1, q_2, q_3 ) \le \tag{Chain rule}\\  
			& \le  H(q_1) + H(q_2) + H(q_3) + 0
\end{align*}

Так как $ H(q_i) \le  \log 3$, для все $ i$ выполнено равенство.

Чтобы было так, мы должны в каждый ход равновероятно получать все три ответа.
Пусть мы взвешиваем кучки из $ k$ монет\footnote{Очевидно, что если взвешивать кучки разного размера, информацию извлечь не получиться даже по Хартли}. 
Вероятность того, что левая кучка будет легче в результате первого взвешивания $\frac{2k}{27}$ (так как у нас либо в левой кучке фальшивая, и она легче, либо в правой кучке фальшивая, и она тяжелее). Тогда $\frac{2k}{27} = \frac{1}{3}$. $k$ получилось нецелым.
Противоречие.

\subsection{Оценка на биноминальные коэффициенты}
\[
	\sum_{i=0}^{k} {n \choose i} \le  2^{n H(\frac{k}{n})}
.\] 
Обозначим сумму за $ C$.

Будем выбирать множество размера не больше $ k$ равновероятно, а затем проверять, попало ли  $ i$ наше множество.
Пусть $ X_i$ --- индикатор того, что $ i$ выбрали. 
\begin{align*}
	\log C = H(X) &\le  H( X_1, \ldots , X_n) \le \\ \tag{Chain rule}
				  & \le \sum_{ }^{ } H( X_i \mid X_{<i}) \le \\
				  & \le \sum_{  }^{ } H(X_i) =  n H(X_1) \le  \tag{считаем, что $ k \le \frac{n}{2}$} \\ 
				  & \le n H\left( \tfrac{k}{n} \right)  
\end{align*}
\textit{Пояснение к последнему переходу.} $H(\frac{k}{n}) = \frac{k}{n} \log \frac{n}{k} + (1 - \frac{k}{n}) \log \frac{1}{1 - \frac{k}{n}}$. С другой стороны $H(X_1) = p \log \frac{1}{p} + (1-p) \log \frac{1}{1-p}$, где $p$~--- вероятность того, что первый элемент попал в множество. Заметим, что функция $f(x) = x \log \frac{1}{x} + (1 - x) \log \frac{1}{1-x}$ возрастает на $(0, \frac{1}{2})$. То есть нужно лишь показать, что $p \le \frac{k}{n}$. Для этого посчитаем матожидание числа элементов, которые вошли в выбранное множество.
\begin{align*}
    k \ge \E(N) = \E(\sum X_i) = \sum \E(X_i) = \sum p_i
\end{align*}
Но каждый элемент войдет в множество равновероятно, тогда $p \le \frac{k}{n}$.

